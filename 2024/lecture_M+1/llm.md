# Лекция: Настройка LLM – от базовых концепций до современных методов оптимизации

## 1. Введение

**Обзор LLM и их применения:**  
Языковые модели (LLM) обучаются на огромных корпусах текста и могут генерировать осмысленные ответы, переводы, резюме и т.д. Применяются в чат-ботах, системах поддержки, генерации контента и исследованиях.

**Зачем нужна настройка поведения модели:**  
Базовая модель может генерировать некорректные или нежелательные ответы. Настройка позволяет:
- Адаптировать поведение под специфику задачи.
- Включать обратную связь (например, человеческую оценку) для повышения качества.
- Контролировать случайность генерации (через temperature и sampling).

**Связь между обучением, подсказками и оптимизацией с обратной связью:**  
- **Zero-shot/Few-shot:** задание задачи без или с небольшим числом примеров.
- **Temperature и Sampling:** контролируют разнообразие выходного текста.
- **RLHF, DPO, PPO, GRPO:** методы настройки модели с учётом предпочтений через обратную связь и алгоритмы обучения с подкреплением.

---

## 2. Основы работы языковых моделей

### 2.1 Zero-shot и Few-shot

**Zero-shot:**  
*Определение:* Модель сразу отвечает на задачу, полагаясь на предобученные знания.  
*Пример:*  
Запрос «Что такое квантовая запутанность?» без примеров. Модель генерирует ответ, опираясь на ранее изученные данные.

**Few-shot:**  
*Определение:* Модель получает 1–10 примеров, чтобы понять задачу.  
*Пример:*  
- **Пример 1:**  
  **Вопрос:** Сколько будет 2+2?  
  **Ответ:** 4  
- **Пример 2:**  
  **Вопрос:** Сколько будет 3+5?  
  **Ответ:** 8  
- **Запрос:** Сколько будет 7+6?  
Модель, увидев примеры, вычисляет 7+6 = 13.

**Математическая формула для вероятности ответа:**  
\[
P(y|x) = \frac{\exp\big(\text{logit}(y|x)\big)}{\sum_{y'} \exp\big(\text{logit}(y'|x)\big)}
\]
где:
- \( \text{logit}(y|x) \) – неотмасштабированное значение для ответа \( y \) при условии запроса \( x \).

*Пример пояснения:* Если у модели для двух ответов логиты равны \( \text{logit}(y_1|x)=3 \) и \( \text{logit}(y_2|x)=1 \), то вероятность \( y_1 \) получается как 
\[
P(y_1|x)=\frac{e^3}{e^3+e^1}\,,
\]
а \( y_2 \) – аналогично.

---

### 2.2 Temperature и стратегии Sampling

**Temperature:**  
*Определение:* Параметр \( T \) контролирует «остроту» распределения вероятностей при выборе следующего токена.  
**Формула:**
\[
P(y_i) = \frac{\exp\left(\frac{\text{logit}_i}{T}\right)}{\sum_j \exp\left(\frac{\text{logit}_j}{T}\right)}
\]
где:
- \( \text{logit}_i \) – логит токена \( y_i \);
- \( T \) – параметр temperature.

*Пример:*  
Если \( T=0.5 \), разница между логитами увеличивается, и наиболее вероятные токены становятся ещё более доминирующими. При \( T=2 \) распределение выравнивается, что позволяет выбирать менее вероятные варианты.

**Стратегии Sampling:**

1. **Greedy Sampling:**  
   - Выбирается токен с максимальной вероятностью на каждом шаге.  
   *Пример:* При последовательном выборе всегда берётся токен с наибольшим значением \( P(y_i) \).

2. **Beam Search:**  
   - Вместо однократного выбора сохраняется «лучшие» \( k \) вариантов (beam width).  
   - **Формула оценки последовательности:**  
     \[
     \text{score}(y_{1:t}) = \sum_{i=1}^{t} \log P(y_i|y_{1:i-1}, x)
     \]
   - *Пример:* При beam width \( k=3 \) на каждом шаге алгоритм отслеживает 3 наиболее перспективные последовательности. После генерации окончательной последовательности выбирается та, у которой максимальная сумма логарифмов вероятностей.

3. **Top-k Sampling:**  
   - Из множества токенов выбирается \( k \) самых вероятных, затем случайно выбирается один из них.  
   *Пример:* При \( k=10 \) из 10 вариантов выбирается один случайно с учетом их вероятностей.

4. **Nucleus (Top-p) Sampling:**  
   - Выбирается минимальное множество токенов, суммарная вероятность которых составляет не менее \( p \) (например, \( p=0.9 \)).  
   *Пример:* Если первые 5 токенов в сумме дают 91% вероятности, выбор происходит только среди них.

---

### 2.3 Chain-of-Thought (CoT) и Internal Reasoning

**Chain-of-Thought (CoT):**  
*Определение:* Модель выводит промежуточные рассуждения перед окончательным ответом.  
*Пример:*  
При решении задачи:
1. Определить, что требуется сложить два числа.
2. Провести операцию сложения.
3. Вывести ответ.

**Internal Reasoning:**  
*Определение:* Внутренний процесс рассуждения, который не выводится пользователю, но влияет на конечный ответ.  
*Пример:* Модель может генерировать промежуточные вычисления, не показывая их в финальном выводе.

---

## 3. Настройка моделей с учётом предпочтений

### 3.1 Preference Tuning с примерами аннотаций

**Что такое Preference Tuning:**  
Модель обучается на парах примеров, где аннотаторы (люди) указывают, какой ответ предпочтительнее.

*Пример аннотаций в RLHF:*  
Представим, что для запроса «Объясни принцип работы двигателя» эксперт аннотирует три ответа:
- **Ответ A:** Подробное объяснение с примерами (аннотируется как лучший, \( y^+ \)).
- **Ответ B:** Слишком краткое объяснение (аннотируется как средний).
- **Ответ C:** Неточное объяснение с ошибками (аннотируется как худший, \( y^- \)).

**Цель:** Обучить модель так, чтобы вероятность генерации ответа A была выше, чем ответа C.

**Функция потерь (пример):**
\[
\mathcal{L} = -\log \sigma\Bigl(\log P(y^+|x) - \log P(y^-|x)\Bigr)
\]
где:
- \( P(y|x) \) – вероятность ответа \( y \) при условии запроса \( x \);
- \( \sigma(z)=\frac{1}{1+e^{-z}} \) – сигмовидная функция.

*Пояснение на примере:* Если \( \log P(y^+|x)= -2 \) и \( \log P(y^-|x)=-3 \), разница \( 1 \) приводит к \( \sigma(1) \approx 0.73 \), что означает, что модель «отдает предпочтение» \( y^+ \). Цель – увеличить эту разницу.

---

### 3.2 RLHF (Reinforcement Learning from Human Feedback)

**Общая идея:**  
Модель обучается с использованием наград, сформированных на основе человеческих аннотаций.

**Этапы RLHF:**
1. **Сбор обратной связи:**  
   Аннотаторы оценивают варианты ответов (например, ранжируют A, B, C, как в примере выше).

2. **Обучение с подкреплением:**  
   Формируется функция награды \( R(s,a) \), где:
   - \( s \) – состояние (например, текст запроса).
   - \( a \) – действие (сгенерированный ответ).

**Формальная цель:**
\[
J(\theta) = \mathbb{E}_{s\sim \pi_\theta}\bigl[ R(s,a) \bigr]
\]
*Что означает \( \mathbb{E}_{s\sim \pi_\theta} \)?*  
Это математическое ожидание (среднее значение) по всем состояниям \( s \), распределённым согласно политике \( \pi_\theta \) (то есть, \( s \) выбираются с вероятностью, определяемой моделью).

**Метод REINFORCE:**  
REINFORCE – классический алгоритм policy gradient, предложенный Уильямсом (1992). Он оценивает градиент:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{s,a \sim \pi_\theta}\Bigl[\nabla_\theta \log \pi_\theta(a|s)\, R(s,a)\Bigr]
\]
где:
- \( \mathbb{E}_{s,a \sim \pi_\theta} \) – ожидание по парам состояний и действий, выбранных согласно политике \( \pi_\theta \);
- \( \nabla_\theta \log \pi_\theta(a|s) \) – градиент логарифма вероятности выбора действия \( a \) в состоянии \( s \);
- \( R(s,a) \) – полученная награда.

*Пример расчёта:*  
Если для конкретного \( (s,a) \) модель имеет вероятность \( \pi_\theta(a|s)=0.2 \), а градиент \( \nabla_\theta \log \pi_\theta(a|s) \) вычислен, то вклад в общий градиент будет пропорционален \( 0.2 \times R(s,a) \). Таким образом, если ответ хороший (большая награда), параметры обновляются в сторону увеличения \( \pi_\theta(a|s) \).

---

## 4. Методы обучения с подкреплением для настройки LLM

### 4.1 PPO (Proximal Policy Optimization)

**Идея PPO:**  
PPO стабилизирует обучение, ограничивая изменение политики между обновлениями.

**Основная формула:**

1. **Отношение вероятностей:**
   \[
   r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
   \]
   *Пример:* Если старая политика даёт вероятность 0.1 для выбранного токена, а новая – 0.12, то \( r_t(\theta)=1.2 \). Это отношение показывает, насколько изменилась вероятность выбора данного действия.

2. **Функция потерь с обрезкой:**
   \[
   L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[\min\Bigl( r_t(\theta) A_t,\; \text{clip}\bigl(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\bigr) A_t \Bigr)\right]
   \]
   где:
   - \( A_t \) – оценка преимущества, которая показывает, насколько выбранное действие лучше средней оценки (например, \( A_t = Q(s_t,a_t) - V(s_t) \) или вычисляется через метод Generalized Advantage Estimation).
   - \( \epsilon \) – гиперпараметр (например, 0.2), ограничивающий шаг обновления.

*Почему используется отношение \( r_t(\theta) \)?*  
Оно позволяет оценить относительное изменение вероятности между новой и старой политиками. Если отношение слишком сильно отличается от 1, обрезка (clip) предотвращает чрезмерное обновление, сохраняя стабильность обучения.

**Как получается \( A_t \) (оценка преимущества):**  
Обычно вычисляется как разность:
\[
A_t = Q(s_t, a_t) - V(s_t)
\]
где:
- \( Q(s_t, a_t) \) – оценка суммарной награды за выбор действия \( a_t \) в состоянии \( s_t \);
- \( V(s_t) \) – базовая оценка (baseline) состояния \( s_t \).  
Также часто применяется метод Generalized Advantage Estimation (GAE):
\[
A_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l\, \delta_{t+l}
\]
с
\[
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
\]
где \( \gamma \) – коэффициент дисконтирования, а \( \lambda \) – параметр, регулирующий bias-variance компромисс.

---

### 4.2 DPO (Direct Preference Optimization)

**Основная идея DPO:**  
DPO оптимизирует модель напрямую на основе пар предпочтений, обходя полный цикл RL. Вместо того чтобы оценивать награды для каждого шага, оптимизируется разность логитов между предпочтительным и непредпочтительным ответами.


**Формула DPO:**

Параметр \( \beta \) регулирует **силу влияния предпочтений** на обновление модели, а не температурное масштабирование. Корректная интерпретация:  
\[
\mathcal{L}_{\text{DPO}} = -\log \sigma\Bigl(\beta \cdot \Delta \Bigr), \quad \Delta = \log \frac{P_\theta(y^+|x)}{P_\theta(y^-|x)}
\]  
где:
- \( \beta > 0 \) — гиперпараметр, определяющий, насколько сильно модель должна предпочитать \( y^+ \) перед \( y^- \).

**Пример:**  
При \( \beta=2 \) и \( \Delta=1 \):  
\[
\mathcal{L}_{\text{DPO}} = -\log \sigma(2 \cdot 1) = -\log \frac{1}{1 + e^{-2}} \approx 0.126
\]  
Это означает, что модель получит меньший штраф, если \( \Delta \) будет большим.

---

## 4.3 GRPO (Group Relative Policy Optimization)

**Основная идея GRPO:**  
GRPO (*Group Relative Policy Optimization*) — метод оптимизации политики, который **устраняет необходимость в отдельной модели функции ценности** (критике). Вместо этого он использует групповое сравнение ответов для вычисления относительных преимуществ. Это позволяет снизить вычислительные затраты и упростить архитектуру обучения.

**Ключевые особенности:**
1. **Групповые сравнения:**  
   На каждой итерации генерируется группа из \( G \) ответов (например, \( G=5 \)) для одного запроса \( x \).  
   *Пример:* Для запроса «Объясни квантовую запутанность» модель генерирует 5 вариантов ответов разной длины и детализации.

2. **Автоматическая оценка:**  
   Каждому ответу \( y_i \) присваивается балл \( r(y_i) \) через **автоматическую функцию награды** (например, длина ответа, соответствие формату, наличие ключевых терминов).  
   *Пример:*  
   \( r(y_1) = 0.9 \) (подробный ответ),  
   \( r(y_2) = 0.5 \) (краткий ответ),  
   \( r(y_3) = 0.2 \) (ответ с ошибками).

3. **Относительное преимущество:**  
   Преимущество \( \hat{A}_i \) для ответа \( y_i \) вычисляется как разница между его баллом и средним баллом группы:  
   \[
   \hat{A}_i = r(y_i) - \frac{1}{G} \sum_{j=1}^G r(y_j)
   \]  
   *Пример:* Если средний балл группы \( 0.5 \), то для \( y_1 \) преимущество будет \( 0.9 - 0.5 = 0.4 \).

**Функция потерь GRPO:**  
\[
\mathcal{L}_{\text{GRPO}} = -\frac{1}{G} \sum_{i=1}^G \left[ \frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)} \hat{A}_i \right] + \beta \, \text{KL}\Bigl[\pi_\theta(\cdot|x) \Vert \pi_{\text{ref}}(\cdot|x)\Bigr]
\]  
где:
- \( \pi_{\text{ref}} \) — эталонная политика (обычно исходная предобученная модель);
- \( \beta \) — коэффициент регуляризации (например, \( \beta=0.1 \)).

**Пояснение формулы:**
- Первый член усиливает ответы с положительным преимуществом (лучше среднего по группе).
- Второй член (KL-дивергенция) ограничивает отклонение новой политики от эталонной, сохраняя стабильность.

**Пример вычислений:**  
Пусть для ответа \( y_1 \):  
- \( \pi_{\text{ref}}(y_1|x) = 0.1 \),  
- \( \pi_\theta(y_1|x) = 0.15 \),  
- \( \hat{A}_1 = 0.4 \).  
Тогда вклад в потерю:  
\[
\frac{0.15}{0.1} \times 0.4 = 0.6
\]  
Модель будет стремиться увеличить \( \pi_\theta(y_1|x) \), так как это снижает общие потери.

**Сравнение с PPO и DPO:**  
| **Метод** | **Требует критика?** | **Режим обучения** | **Особенность**  
|-----------|-----------------------|--------------------|-----------------  
| PPO       | Да                    | Оффлайн/Онлайн     | Использует обрезку (clip)  
| DPO       | Нет                   | Оффлайн            | Оптимизирует парные предпочтения  
| GRPO      | Нет                   | Онлайн             | Использует групповые сравнения  

---


### 4.3.1 Chain-of-Thought (CoT) и GRPO (дополнение)

**Влияние GRPO на генерацию рассуждений:**  
GRPO может **стимулировать CoT даже без явных примеров** в обучающих данных. Например, если функция награды \( r(y) \) учитывает наличие логических шагов (например, счётчик «потому что», «следовательно»), модель начнёт генерировать промежуточные рассуждения:  
```python
def reward_function(y):
    cot_keywords = ["потому что", "следовательно", "шаг 1"]
    return sum(1 for keyword in cot_keywords if keyword in y)
```  
*Результат:* Модель автоматически учится добавлять CoT, чтобы максимизировать награду.
