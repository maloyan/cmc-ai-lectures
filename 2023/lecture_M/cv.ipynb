{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b938bbe4-5351-48fd-abc4-886c7e99d88f",
   "metadata": {},
   "source": [
    "# Компьютерное зрение\n",
    "\n",
    "Компьютерное зрение — это раздел искусственного интеллекта, который занимается интерпретацией и анализом изображений и видео для выполнения разнообразных задач, подражая человеческому зрению. Вот неполный список задач компьютерного зрения:\n",
    "\n",
    "1. **Распознавание объектов (Object Recognition)**: Определение и классификация объектов на изображении или видео, как, например, в системах распознавания лиц для обеспечения безопасности.\n",
    "\n",
    "2. **Детекция объектов (Object Detection)**: Локализация объектов на изображении с помощью ограничивающих рамок, используется в автономных транспортных средствах для обнаружения пешеходов и других автомобилей.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:872/0*ByD9YgAm-H--o70v.png)\n",
    "\n",
    "3. **Сегментация изображений (Image Segmentation)**: Разделение изображения на сегменты для анализа отдельных объектов и областей, применяется в медицинской визуализации для выделения опухолей.\n",
    "\n",
    "![](https://www.researchgate.net/publication/373042406/figure/fig1/AS:11431281180742766@1691685922004/Example-of-Image-Segmentation-in-medical-imaging.png)\n",
    "\n",
    "4. **Отслеживание объектов (Object Tracking)**: Слежение за перемещением объектов в видеопотоке, важно для систем видеонаблюдения и анализа спортивных игр.\n",
    "\n",
    "![](https://aidetic.in/blog/wp-content/uploads/2020/10/mulitple_objects_tracking_525x350.jpg)\n",
    "\n",
    "5. **Распознавание жестов (Gesture Recognition)**: Интерпретация человеческих жестов через изображения или видео для интерактивного управления устройствами.\n",
    "\n",
    "![](https://media.licdn.com/dms/image/D4D12AQEqDr2WmIDKrg/article-cover_image-shrink_600_2000/0/1690389776221?e=2147483647&v=beta&t=ySugcnbIEAOTVmuhHPjdxlBl1ez7uvnwQL520TQGVNU)\n",
    "\n",
    "6. **Оценка позы человека (Human Pose Estimation)**: Определение поз и движений человека, применяется в спортивной аналитике и разработке интерактивных игр.\n",
    "\n",
    "![](https://nanonets.com/blog/content/images/2019/04/Screen-Shot-2019-04-11-at-5.17.56-PM.png)\n",
    "\n",
    "7. **Реконструкция сцены (Scene Reconstruction)**: Создание трехмерных моделей сцен из двумерных изображений или видео, что является ключевым для создания виртуальной реальности и аугментированной реальности.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/tasks/Screen_Shot_2020-07-21_at_4.39.24_PM_zR4IJYi.png)\n",
    "\n",
    "9. **Оценка глубины (Depth Estimation)**: Определение расстояния до объектов на изображении для создания трехмерных моделей сцен или объектов, ключевая задача в робототехнике и автономном вождении.\n",
    "\n",
    "![](https://repository-images.githubusercontent.com/184593965/1d3e0500-6cf2-11e9-8a90-7fbed404dc88)\n",
    "\n",
    "Эти задачи подчеркивают широкий спектр применений компьютерного зрения в различных областях, от безопасности и медицины до автомобильной индустрии и развлечений, включая важные аспекты трехмерного анализа и интерпретации данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81382d6-baec-4552-94bf-dac1b6805846",
   "metadata": {},
   "source": [
    "## Задача сегментации в компьютерном зрении\n",
    "\n",
    "### Формальная постановка задачи\n",
    "\n",
    "Задача сегментации изображений заключается в разделении изображения на множество сегментов (или пикселей), которые группируются по определенному признаку или критерию. Цель состоит в том, чтобы каждый пиксель был отнесен к определенному классу, что позволяет более детально анализировать изображение, выделяя конкретные объекты, формы или текстуры.\n",
    "\n",
    "Математически задачу можно сформулировать следующим образом: \n",
    "\n",
    "Для данного изображения $I$ размером $W \\times H$ (где $W$ - ширина, $H$ - высота) необходимо каждому пикселю $p_{ij}$, где $i \\in 1, \\ldots, W$, $j \\in 1, \\ldots, H$, присвоить метку класса $c \\in C$, где $C$ - набор возможных классов. В результате получается карта сегментации $S$, где каждому пикселю изображения $I$ соответствует метка класса из $C$.\n",
    "\n",
    "![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/616b648b00d9595e571bd77b_an-overview-of-semantic-image-segmentation%20(1).png)\n",
    "\n",
    "### Обучающие данные\n",
    "\n",
    "Обучающие данные для задачи сегментации обычно состоят из набора изображений и соответствующих им карт сегментации. Карта сегментации - это изображение, где каждому пикселю присвоена метка класса, указывающая на принадлежность пикселя к определенному объекту или фону. В идеале, обучающий набор должен быть разнообразным и покрывать все классы объектов, которые модель должна уметь распознавать, а также различные условия освещения, ракурсы и фоны.\n",
    "\n",
    "### Виды кодирования результата\n",
    "\n",
    "1. **Попиксельное кодирование (Pixel-wise encoding)**: Каждому пикселю присваивается уникальная метка класса. Это наиболее детальное кодирование, позволяющее точно определить границы объектов.\n",
    "\n",
    "2. **Кодирование через карту вероятностей (Probability map encoding)**: Для каждого пикселя модель выдает вектор вероятностей принадлежности к каждому из классов. Этот метод позволяет оценить неопределенность модели в отношении каждого пикселя и выбрать наиболее вероятный класс.\n",
    "\n",
    "3. **Кодирование через инстансы (Instance encoding)**: Применяется в задачах сегментации на уровне инстансов, где каждому индивидуальному объекту присваивается уникальный идентификатор, даже если объекты принадлежат одному классу. Это позволяет различать отдельные инстансы одного класса на изображении.\n",
    "\n",
    "Каждый из этих методов кодирования имеет свои преимущества и недостатки и может быть выбран в зависимости от конкретной задачи и требований к точности и детализации результатов сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361beca-7b88-4088-af1d-c46192e5af87",
   "metadata": {},
   "source": [
    "Для оценки качества моделей семантической сегментации используются различные метрики. Каждая метрика имеет свои преимущества и недостатки, и выбор конкретной метрики зависит от специфики задачи и требований к результатам. Ниже приведены наиболее часто используемые метрики для семантической сегментации, их формулы, а также преимущества и недостатки.\n",
    "\n",
    "### 1. Точность (Accuracy)\n",
    "\n",
    "Точность - это одна из самых базовых метрик, используемых для оценки моделей семантической сегментации. Она вычисляется как отношение числа правильно классифицированных пикселей ко всем пикселям изображения.\n",
    "\n",
    "**Преимущества**:\n",
    "- Простота понимания и вычисления.\n",
    "\n",
    "**Недостатки**:\n",
    "- Не учитывает дисбаланс классов, что делает её малоинформативной для неравномерно распределенных данных.\n",
    "\n",
    "### 2. IoU (Intersection over Union)\n",
    "\n",
    "IoU (пересечение по объединению) - это метрика, используемая для измерения перекрытия между предсказанной маской сегментации и истинной маской. IoU вычисляется для каждого класса отдельно, а затем может быть усреднена по всем классам для получения метрики mIoU (mean IoU).\n",
    "\n",
    "Формула IoU:\n",
    "$ \\text{IoU} = \\frac{TP}{TP + FP + FN} $\n",
    "\n",
    "**Преимущества**:\n",
    "- Учитывает как размер объектов, так и их взаимное расположение, обеспечивая более сбалансированную оценку.\n",
    "\n",
    "**Недостатки**:\n",
    "- Может быть неинформативной для объектов малого размера из-за доминирования ошибок в больших объектах.\n",
    "\n",
    "![](https://idiotdeveloper.com/wp-content/uploads/2023/01/iou-1024x781.webp)\n",
    "\n",
    "### 3. Dice Coefficient\n",
    "\n",
    "Коэффициент Dice, аналогичный F1-мере, используется для измерения сходства между двумя множествами и часто применяется в медицинской сегментации.\n",
    "\n",
    "Формула коэффициента Dice:\n",
    "$ \\text{Dice} = \\frac{2TP}{2TP + FP + FN} $\n",
    "\n",
    "**Преимущества**:\n",
    "- Хорошо подходит для задач, где объекты сегментации занимают небольшую часть изображения.\n",
    "\n",
    "**Недостатки**:\n",
    "- Аналогично F1-мере, может быть менее информативен для многоклассовой сегментации без должной адаптации.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*tSqwQ9tvLmeO9raDqg3i-w.png)\n",
    "\n",
    "Выбор метрики зависит от специфических требований задачи и данных. Важно учитывать как статистические показатели качества, так и практическую значимость метрик для конкретного приложения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790adf36-6fb2-4763-994c-c721cd2f333c",
   "metadata": {},
   "source": [
    "# Методы семантической сегментации\n",
    "\n",
    "## 1. Sliding window\n",
    "\n",
    "![](https://oi.readthedocs.io/en/latest/_images/ss_sliding_window.png)\n",
    "\n",
    "Метод скользящего окна (sliding window) с классификацией центрального пикселя является одним из подходов, применяемых в задачах компьютерного зрения, в частности, в сегментации изображений и распознавании объектов. Этот метод объединяет в себе идеи как из классической обработки изображений, так и из машинного обучения, и может быть использован для извлечения признаков или непосредственной классификации.\n",
    "\n",
    "### Принцип работы\n",
    "\n",
    "1. **Определение окна**: Выбирается окно фиксированного размера, которое \"скользит\" по всему изображению. Размер окна определяется исходя из специфики задачи и может варьироваться от небольших окон (например, 3x3, 5x5 пикселей) до более крупных (например, 32x32, 64x64 пикселей). \n",
    "\n",
    "2. **Просмотр изображения**: Окно последовательно перемещается по изображению с некоторым шагом (stride), который также может быть задан в зависимости от требуемой детализации и вычислительных мощностей. Для каждой позиции окна производится анализ содержащихся в нем пикселей.\n",
    "\n",
    "3. **Классификация центрального пикселя**: В фокусе внимания находится центральный пиксель каждого окна. Информация о пикселях внутри окна используется для извлечения признаков или непосредственной классификации центрального пикселя. Например, может применяться классификатор (например, SVM, случайный лес или нейронная сеть), который на основе этих данных определяет класс центрального пикселя.\n",
    "\n",
    "## 2. Fully convolutional\n",
    "\n",
    "![](https://oi.readthedocs.io/en/latest/_images/ss_fcn_01.png)\n",
    "\n",
    "### Принцип работы\n",
    "\n",
    "1. **Извлечение признаков**: Сверточные слои автоматически извлекают из изображений признаки разного уровня абстракции. На начальных слоях сети обнаруживаются простые признаки, такие как края и углы, а на более глубоких слоях — более сложные структуры и паттерны.\n",
    "\n",
    "3. **Классификация пикселей**: В отличие от классификации всего изображения, в задачах сегментации выходной слой сети обычно возвращает карту признаков, где каждый пиксель классифицируется на один из целевых классов. Для этого может использоваться, например, слой с операцией softmax на выходе, который предсказывает класс для каждого пикселя отдельно.\n",
    "\n",
    "\n",
    "## Unet\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/0*FZ-llYVQXTPc8GSe)\n",
    "\n",
    "U-Net является одной из наиболее популярных архитектур сверточной нейронной сети для задачи семантической сегментации, особенно в области медицинской визуализации. Эта модель была представлена Olaf Ronneberger, Philipp Fischer и Thomas Brox в 2015 году в работе, посвященной сегментации биомедицинских изображений. Основная идея U-Net заключается в точной локализации и классификации пикселей изображения, что делает ее особенно подходящей для задач, где необходимо выделять сложные структуры на изображениях с высокой точностью.\n",
    "\n",
    "### Структура U-Net\n",
    "\n",
    "Архитектура U-Net характеризуется U-образной формой и состоит из двух основных частей: пути сжатия (сверточная часть, \"encoder\") и пути расширения (де-сверточная часть, \"decoder\").\n",
    "\n",
    "1. **Путь сжатия**: Состоит из повторяющихся блоков, каждый из которых включает в себя две свертки с активацией ReLU и последующим пулингом для уменьшения размерности. Эта часть сети отвечает за извлечение и уменьшение признаков из входного изображения, позволяя сети \"понимать\" контекст изображения.\n",
    "\n",
    "2. **Путь расширения**: Состоит из блоков, которые увеличивают размерность признаков с помощью операций де-свертки (или обратного пулинга). Каждый шаг в пути расширения включает в себя де-свертку, конкатенацию с соответствующими признаковыми картами из пути сжатия и последующие свертки. Это обеспечивает передачу контекста и локализационной информации для точной сегментации.\n",
    "\n",
    "### Принцип работы Unpooling\n",
    "\n",
    "Во время операции пулинга, например, max pooling, сохраняется максимальное значение из определенной области входной карты признаков, в то время как остальная информация теряется. Unpooling пытается восстановить размерность исходной карты признаков, «разворачивая» операцию пулинга.\n",
    "\n",
    "Существуют различные стратегии для выполнения операции unpooling:\n",
    "\n",
    "1. **Max-Unpooling**: В этом методе в процессе max pooling запоминается расположение максимальных значений (индексы максимальных элементов). Во время unpooling эти индексы используются для размещения значений в увеличенной карте признаков, гарантируя, что значения восстанавливаются в правильных местоположениях. Остальные позиции обычно заполняются нулями.\n",
    "\n",
    "![](https://www.researchgate.net/publication/306081538/figure/fig2/AS:418518853013507@1476794078414/Pooling-and-unpooling-layers-For-each-pooling-layer-the-max-locations-are-stored-These.png)\n",
    "\n",
    "2. **Nearest Neighbor Unpooling**: Этот метод просто копирует значение в несколько соседних позиций в увеличенной карте признаков, эффективно увеличивая размер карты без использования информации о расположении из операции пулинга.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1364/1*9N9FVYalaVAk-aalcBVYaA.png)\n",
    "\n",
    "3. **Bed of Nails Unpooling**: Подобно max-unpooling, но без сохранения индексов максимальных значений. Вместо этого выбирается фиксированная схема для распределения значений, что приводит к менее точному восстановлению.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1384/1*LQVVqK8YJ4ndcU6NS4UOxA.png)\n",
    "\n",
    "### Особенности\n",
    "\n",
    "- **Skip-connections (соединения пропуска)**: Одной из ключевых особенностей U-Net являются соединения пропуска, которые напрямую соединяют блоки пути сжатия с соответствующими блоками пути расширения. Эти соединения передают информацию о местоположении и позволяют комбинировать признаки высокого и низкого уровней, что способствует улучшению точности сегментации.\n",
    "\n",
    "- **Эффективное использование данных**: Благодаря своей структуре, U-Net способна эффективно работать даже с ограниченным количеством обучающих данных, что делает ее особенно ценной в областях, где размеченные данные труднодоступны, как, например, в медицинских приложениях.\n",
    "\n",
    "### Применение\n",
    "\n",
    "U-Net нашла широкое применение не только в медицинской визуализации для сегментации различных типов тканей, опухолей и других анатомических структур, но и в других областях, требующих точной сегментации, включая анализ спутниковых снимков, детекцию дорожных знаков и многое другое.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*f7YOaE4TWubwaFF7Z1fzNw.png)\n",
    "\n",
    "\n",
    "### Вариации\n",
    "\n",
    "1. Unet++\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*jOZxlxGRWhvQzDCtxfwmfQ.png)\n",
    "\n",
    "2. TernausNet\n",
    "\n",
    "![](https://camo.githubusercontent.com/949eb3ec348109aa57cb55752e74a5a465cca65250e34f5d587885fdeae69b97/68747470733a2f2f686162726173746f726167652e6f72672f776562742f68752f6a692f69722f68756a696972767067706637657377713838685f783761686c69772e706e67)\n",
    "\n",
    "И др., подробнее можно увидеть здесь [Segmentation models](https://github.com/qubvel/segmentation_models.pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b4289-c1ab-4121-9c9f-184ff0558f13",
   "metadata": {},
   "source": [
    "# Функции потерь для сегментации\n",
    "\n",
    "В задачах сегментации изображений выбор функции потерь играет ключевую роль, поскольку он напрямую влияет на способность модели точно разделять различные классы объектов на изображении. Вот несколько наиболее распространенных функций потерь, используемых для сегментации:\n",
    "\n",
    "### 1. Перекрестная энтропия (Cross-Entropy Loss)\n",
    "\n",
    "Перекрестная энтропия — это стандартная функция потерь для задач классификации, которая также широко используется в сегментации для измерения разницы между предсказанными и истинными распределениями классов на уровне пикселей.\n",
    "\n",
    "$ L_{CE} = -\\sum_{c=1}^{M} y_{o,c} \\log(p_{o,c}) $\n",
    "\n",
    "где $M$ — количество классов, $y$ — бинарный индикатор (0 или 1), указывающий, принадлежит ли пиксель классу $c$, и $p$ — предсказанная вероятность того, что пиксель принадлежит классу $c$.\n",
    "\n",
    "**Преимущества**: Простота реализации и эффективность во многих случаях.\n",
    "\n",
    "**Недостатки**: Может быть неэффективной при наличии дисбаланса классов, поскольку модель может сосредоточиться на более часто встречающихся классах.\n",
    "\n",
    "### 2. Dice Loss\n",
    "\n",
    "Dice Loss основан на коэффициенте Dice, который является мерой перекрытия между двумя образцами. Эта функция особенно полезна для задач сегментации, где классы объектов неравномерно распределены.\n",
    "\n",
    "$ L_{Dice} = 1 - \\frac{2 \\sum_{i=1}^{N} p_i y_i}{\\sum_{i=1}^{N} p_i^2 + \\sum_{i=1}^{N} y_i^2} $\n",
    "\n",
    "где $p_i$ и $y_i$ — предсказанные вероятности и истинные метки для каждого пикселя соответственно, а $N$ — общее количество пикселей.\n",
    "\n",
    "**Преимущества**: Хорошо справляется с дисбалансом классов.\n",
    "\n",
    "**Недостатки**: Может быть сложнее сходиться, особенно в начале обучения, когда перекрытие между предсказаниями и истинными метками минимально.\n",
    "\n",
    "### 3. Функция потерь Jaccard (Jaccard Loss или IoU Loss)\n",
    "\n",
    "Функция потерь Jaccard аналогична Dice Loss и измеряет сходство и различие между наборами пикселей.\n",
    "\n",
    "$ L_{Jaccard} = 1 - \\frac{\\sum_{i=1}^{N} p_i y_i}{\\sum_{i=1}^{N} p_i + \\sum_{i=1}^{N} y_i - \\sum_{i=1}^{N} p_i y_i} $\n",
    "\n",
    "**Преимущества**: Предоставляет прямую оценку перекрытия между сегментированными областями, что делает ее интуитивно понятной.\n",
    "\n",
    "**Недостатки**: Может быть более чувствительной к дисбалансу классов, чем перекрестная энтропия.\n",
    "\n",
    "### 4. Focal Loss\n",
    "\n",
    "Focal Loss модифицирует перекрестную энтропию для уменьшения влияния легко классифицируемых объектов на общие потери, увеличивая важность сложных, неправильно классифицированных примеров.\n",
    "\n",
    "$ L_{Focal} = -\\alpha (1 - p_t)^\\gamma \\log(p_t) $\n",
    "\n",
    "где $p_t$ — вероятность принадлежности к целевому классу, $\\alpha$ — коэффициент балансировки, а $\\gamma$ — модулирующий фактор, уменьшающий влияние легких примеров.\n",
    "\n",
    "**Преимущества**: Помогает справиться с дисбалансом классов и улучшает обучение на сложных примерах.\n",
    "\n",
    "**Недостатки**: Необходимость настройки дополнительных гиперпараметров ($\\alpha$ и $\\gamma$).\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1052/1*AxzcROKIM97T_POYJzHlDA.png)\n",
    "\n",
    "### 5. Boundary Loss\n",
    "\n",
    "Boundary Loss — это специализированная функция потерь, разработанная для улучшения качества сегментации на границах объектов в задачах компьютерного зрения. Эта функция потерь фокусируется на точном выделении границ объектов, что особенно важно в медицинской визуализации и других приложениях, где точность границ является критическим фактором.\n",
    "\n",
    "Boundary Loss рассчитывается на основе различий между предсказанными и истинными границами объектов. Один из подходов к реализации этой функции потерь включает использование расстояния между соответствующими точками на предсказанных и истинных границах или применение трансформации расстояния (например, расстояние до ближайшего соседа) для получения меры близости границ.\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S1361841520302152-gr2.jpg)\n",
    "\n",
    "### Комбинированные функции потерь\n",
    "\n",
    "Во многих задачах сегментации эффективность модели может быть значительно улучшена за счет использования комбинированных функций потерь, которые позволяют одновременно минимизировать несколько видов ошибок. Комбинирование функций потерь позволяет учитывать различные аспекты задачи сегментации, например, общую точность классификации пикселей и точность определения границ объектов.\n",
    "\n",
    "#### Примеры комбинированных функций потерь:\n",
    "\n",
    "1. **Dice + Cross-Entropy Loss**:\n",
    "   - Комбинирование Dice Loss и перекрестной энтропии может улучшить обучение, обеспечивая одновременно высокую точность сегментации и устойчивость к дисбалансу классов.\n",
    "\n",
    "2. **Boundary Loss + Cross-Entropy Loss**:\n",
    "   - Добавление Boundary Loss к перекрестной энтропии позволяет модели лучше выделять границы объектов, одновременно сохраняя хорошую общую точность классификации.\n",
    "\n",
    "3. **Focal Loss + Dice Loss**:\n",
    "   - Эта комбинация позволяет модели сосредоточиться как на сложных для классификации примерах, так и на улучшении качества сегментации за счет точности определения границ объектов.\n",
    "\n",
    "![](https://malmic.ca/wp-content/uploads/2022/02/Loss-odyssey-in-medical-image-segmentation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba8eb9-e40a-41da-ab3c-8504a3d97b02",
   "metadata": {},
   "source": [
    "### Задача детекции\n",
    "\n",
    "Задача детекции (обнаружения объектов) в компьютерном зрении состоит в определении местоположения объектов на изображении или видео и классификации этих объектов. Это означает, что модель должна не только распознать объекты различных классов на изображении, но и указать их расположение с помощью ограничивающих рамок (bounding boxes).\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:416/1*Ic-ME4SgJeIgRDZvZu0ivw.jpeg)\n",
    "\n",
    "### Тренировочные данные\n",
    "\n",
    "Тренировочные данные для задачи детекции обычно включают в себя:\n",
    "- Изображения или видео с размеченными объектами.\n",
    "- Аннотации к этим изображениям, указывающие класс объекта и его местоположение на изображении (обычно в виде ограничивающей рамки).\n",
    "\n",
    "\n",
    "#### Стандарты аннотации тренировочных данных\n",
    "\n",
    "Существуют стандартные наборы данных, широко используемые для обучения и тестирования моделей детекции, такие как:\n",
    "- COCO (Common Objects in Context): [x_min, y_min, width, height]\n",
    "    - На примере кот имеет bbox [98, 345, 322, 117]\n",
    "\n",
    "-  YOLO (You Only Look Once): [center_x, center_y, width, height]. \n",
    "\n",
    "    - На примере кот имеет bbox [((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480], что есть [0.4046875, 0.840625, 0.503125, 0.24375]\n",
    "\n",
    "- Pascal VOC (Visual Object Classes): [x_min, y_min, x_max, y_max]\n",
    "\n",
    "- ImageNet: [x_min, y_min, x_max, y_max]\n",
    "\n",
    "![](https://albumentations.ai/docs/images/getting_started/augmenting_bboxes/bbox_example.jpg)\n",
    "\n",
    "### Метрики оценки качества детекции\n",
    "\n",
    "Основные метрики для оценки качества детекции включают точность (precision), полноту (recall) и AP (Average Precision).\n",
    "\n",
    "#### Precision и Recall\n",
    "\n",
    "$ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "\n",
    "$ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "\n",
    "где $TP$ (True Positives) — количество правильно обнаруженных объектов, $FP$ (False Positives) — количество неправильно обнаруженных объектов, и $FN$ (False Negatives) — количество объектов, которые модель не смогла обнаружить.\n",
    "\n",
    "#### Average Precision (AP) и Mean Average Precision (mAP)\n",
    "\n",
    "AP — это средняя точность обнаружения объектов для одного класса, вычисляемая как площадь под кривой Precision-Recall. mAP (Mean Average Precision) вычисляется как среднее значение AP по всем классам.\n",
    "\n",
    "$ AP = \\int_{0}^{1} p(r) \\,dr $\n",
    "\n",
    "где $p(r)$ — функция точности в зависимости от полноты $r$.\n",
    "\n",
    "mAP часто используется в качестве основной метрики для оценки моделей в соревнованиях и научных работах, поскольку она учитывает как точность, так и полноту обнаружения для каждого класса объектов.\n",
    "\n",
    "#### IoU (Intersection over Union)\n",
    "\n",
    "Для оценки точности локализации объектов используется метрика IoU — отношение пересечения ограничивающей рамки, предсказанной моделью, к объединению этой рамки с истинной ограничивающей рамкой.\n",
    "\n",
    "$ IoU = \\frac{\\text{area of overlap}}{\\text{area of union}} $\n",
    "\n",
    "Обычно детекция считается корректной, если IoU превышает определенный порог (например, 0.5).\n",
    "\n",
    "### Заключение\n",
    "\n",
    "Оценка качества моделей детекции требует комплексного подхода, учитывающего как способность модели корректно классифицировать и локализовать объекты, так и общую точность и полноту обнаружения объектов различных классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcbc35-2717-4a9c-91cb-da5e2a5f5ce1",
   "metadata": {},
   "source": [
    "# Методы детекции\n",
    "\n",
    "## Sliding window detection\n",
    "\n",
    "Простейший метод детекции, который использует проход скользящим окном разных размеров по изображению, известен как метод скользящего окна (Sliding Window Detection). Этот метод является одним из самых ранних подходов в области детекции объектов и до сих пор применяется в определённых контекстах, особенно там, где задачи и условия позволяют использовать простые и понятные алгоритмы.\n",
    "\n",
    "### Принцип работы\n",
    "\n",
    "1. **Определение окон**: Определяется набор окон (подокон) различного размера и соотношения сторон, которые будут использоваться для сканирования изображения. Размеры окон выбираются на основе ожидаемого размера объектов на изображении.\n",
    "\n",
    "2. **Проход по изображению**: Каждое окно последовательно \"скользит\" по всему изображению с заданным шагом (stride). В каждой позиции окно \"вырезает\" часть изображения для последующего анализа.\n",
    "\n",
    "3. **Классификация**: Вырезанное изображение подается на вход классификатора, который определяет, содержит ли изображение искомый объект. Классификатор может быть реализован с использованием различных методов машинного обучения, начиная от простых, таких как SVM (Support Vector Machines), до более сложных, включая сверточные нейронные сети (CNN).\n",
    "\n",
    "4. **Обработка результатов**: Для каждого окна, где классификатор обнаруживает объект, фиксируется его местоположение. После прохождения по всему изображению может потребоваться дополнительная обработка результатов, например, методы подавления немаксимумов (non-maximum suppression), для устранения перекрывающихся детекций и уточнения местоположения объектов.\n",
    "\n",
    "### Преимущества и недостатки\n",
    "\n",
    "**Преимущества**:\n",
    "- Простота реализации и понимания.\n",
    "- Возможность использования без необходимости обучения сложных моделей (если используется простой классификатор).\n",
    "\n",
    "**Недостатки**:\n",
    "- Высокая вычислительная сложность, особенно при использовании большого количества окон разных размеров и маленького шага.\n",
    "- Низкая точность и высокий процент ложных срабатываний, особенно в сложных сценах с перекрытием объектов или вариативностью фона.\n",
    "- Ограниченная способность к детекции объектов разного масштаба и в различных ориентациях без значительного увеличения числа используемых окон и, соответственно, вычислительной сложности.\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRCRH4j4UYPr2IXvGk8bt62K0sEtK0eaGsw4qVpjKjgpZ-Mi9n8D4PSC3x4pqD_HQn2Tts&usqp=CAU)\n",
    "\n",
    "## R- CNN\n",
    "R-CNN (Regions with Convolutional Neural Networks) — это один из революционных подходов в задаче детекции объектов, представленный Ross Girshick и его коллегами в 2014 году. R-CNN сочетает в себе два ключевых компонента: предложение регионов, где могут находиться объекты, и использование сверточных нейронных сетей (CNN) для классификации этих предложенных регионов.\n",
    "\n",
    "### Принцип работы R-CNN\n",
    "\n",
    "1. **Предложение регионов**: R-CNN начинает с извлечения около 2000 регионов-кандидатов (region proposals) из входного изображения, которые потенциально содержат объекты. Для этого используется алгоритм, такой как Selective Search, который группирует соседние пиксели по цвету, текстуре и другим признакам для генерации предложений регионов.\n",
    "Selective Search (Выборочный поиск) — это алгоритм, используемый в компьютерном зрении для выделения регионов-кандидатов на изображении, в которых с большой вероятностью могут находиться объекты. Алгоритм был разработан как компромисс между полным перебором всех возможных регионов на изображении, что слишком ресурсоемко, и слишком грубыми методами, которые могут пропустить важные объекты. Selective Search широко используется в задачах детекции объектов, включая подходы, основанные на R-CNN и его производных.\n",
    "\n",
    "    - **Алгоритм Selective Search** основывается на иерархической группировке сегментов изображения для формирования регионов-кандидатов.\n",
    "\n",
    "    1. **Сегментация изображения**: Изображение первоначально сегментируется на множество мелких регионов с использованием простой алгоритмической сегментации, такой как алгоритм быстрой сегментации. Эти первичные сегменты должны быть достаточно малы, чтобы с высокой вероятностью каждый содержал лишь часть объекта или один маленький объект.\n",
    "\n",
    "    2. **Иерархическое объединение**: Начиная с этих мелких сегментов, алгоритм итеративно объединяет их в более крупные на основе различных подобия — цветового, текстурного, размерного подобия и подобия формы. В каждом шаге объединяются наиболее похожие сегменты.\n",
    "\n",
    "    3. **Генерация регионов-кандидатов**: На разных этапах иерархического объединения, различные регионы сохраняются как регионы-кандидаты для последующей проверки. Это позволяет учесть объекты разного масштаба и сложности формы.\n",
    "\n",
    "    4. **Устранение похожих регионов**: Для уменьшения количества регионов-кандидатов применяются методы устранения похожих регионов, чтобы минимизировать избыточность и ускорить последующую обработку.\n",
    "    \n",
    "    ![](https://arthurdouillard.com/figures/selective_search1.png)\n",
    "\n",
    "2. **Извлечение признаков**: Каждый предложенный регион масштабируется до фиксированного размера и подается на вход сверточной нейронной сети для извлечения признаков. Эти признаки служат представлением каждого региона.\n",
    "\n",
    "3. **Классификация регионов**: Извлеченные признаки каждого региона затем классифицируются с использованием набора классификаторов (например, SVM), каждый из которых предназначен для определения, принадлежит ли регион к одному из интересующих классов объектов.\n",
    "\n",
    "4. **Регрессия ограничивающей рамки**: После классификации для каждого региона, который классифицирован как содержащий объект, выполняется регрессия ограничивающей рамки (bounding box regression) для уточнения положения и размеров рамки, чтобы она как можно точнее обрамляла объект.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/new_splash-method_NaA95zW.jpg)\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- **Точность**: R-CNN значительно улучшил точность детекции объектов по сравнению с предыдущими подходами благодаря мощи сверточных нейронных сетей для извлечения признаков и точной локализации объектов.\n",
    "- **Гибкость**: Модель можно легко адаптировать для различных наборов данных и классов объектов, изменяя лишь классификаторы и регрессоры ограничивающих рамок.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- **Скорость**: Из-за необходимости обработки тысяч регионов для каждого изображения, R-CNN работает относительно медленно, что затрудняет его использование в реальном времени.\n",
    "- **Эффективность обучения**: Обучение R-CNN происходит в несколько этапов, что делает процесс сложным и времязатратным. Кроме того, требуется отдельное обучение для сверточной сети, классификаторов и регрессоров ограничивающих рамок.\n",
    "\n",
    "\n",
    "## Fast R-CNN\n",
    "\n",
    "Fast R-CNN является усовершенствованной версией оригинальной модели R-CNN и был представлен Ross Girshick в 2015 году. Основной целью Fast R-CNN было ускорение процесса детекции объектов и упрощение процесса обучения по сравнению с R-CNN. Fast R-CNN достигает этих улучшений через несколько ключевых инноваций.\n",
    "\n",
    "### Основные принципы работы Fast R-CNN\n",
    "\n",
    "1. **Единая сверточная сеть для всего изображения**: В отличие от R-CNN, где для каждого предложенного региона независимо вычисляются свёрточные признаки, Fast R-CNN применяет сверточную сеть один раз ко всему изображению для получения карты признаков. Это значительно уменьшает вычислительные затраты.\n",
    "\n",
    "2. **RoI (Region of Interest) Pooling слой**: Этот слой принимает карту признаков всего изображения и список предложенных регионов (region proposals) и для каждого региона выдает фиксированный набор признаков, независимо от его размера и формы. RoI Pooling позволяет эффективно использовать предварительно вычисленные на всем изображении признаки для каждого региона.\n",
    "\n",
    "3. **Классификация и регрессия ограничивающих рамок одновременно**: Fast R-CNN использует единый набор полносвязных слоев для выполнения как классификации предложенных регионов, так и корректировки их ограничивающих рамок. Это достигается за счет использования двух параллельных выходных слоев: один для предсказания вероятностей принадлежности регионов к классам, а другой для уточнения ограничивающих рамок.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*0pMP3aY8blSpva5tvWbnKA.png)\n",
    "\n",
    "### Преимущества Fast R-CNN\n",
    "\n",
    "- **Высокая скорость обработки**: Благодаря обработке всего изображения один раз, Fast R-CNN работает значительно быстрее, чем оригинальный R-CNN.\n",
    "- **Упрощенное обучение**: Fast R-CNN позволяет обучать детектор объектов и регрессор ограничивающих рамок одновременно, что упрощает процесс обучения.\n",
    "- **Улучшение точности детекции**: Использование более сложных и эффективных архитектур сверточных сетей улучшает точность детекции.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- **Зависимость от качества предложенных регионов**: Точность Fast R-CNN по-прежнему зависит от качества и количества предложенных регионов, которые обычно генерируются с использованием внешних методов, таких как Selective Search.\n",
    "- **Вычислительные затраты на этапе предложения регионов**: Хотя сам Fast R-CNN работает быстро, общее время обработки может быть затратным из-за необходимости предварительной генерации регионов.\n",
    "\n",
    "\n",
    "## Faster R-CNN\n",
    "\n",
    "Faster R-CNN, представленный Shaoqing Ren, Kaiming He, Ross Girshick и Jian Sun в 2015 году, является значительным улучшением в области детекции объектов по сравнению с предшественниками R-CNN и Fast R-CNN. Эта модель интегрирует процесс генерации предложений регионов непосредственно в сеть, что позволяет детектировать объекты в реальном времени с высокой точностью.\n",
    "\n",
    "### Основные компоненты Faster R-CNN\n",
    "\n",
    "Faster R-CNN состоит из двух основных частей: сети предложения регионов (Region Proposal Network, RPN) и сети детекции объектов, которая использует предложенные регионы для детекции и классификации объектов.\n",
    "\n",
    "#### 1. Region Proposal Network (RPN)\n",
    "\n",
    "RPN — это сверточная сеть, которая сканирует карту признаков, полученную из предварительной сверточной сети, и для каждого местоположения предсказывает набор предложений регионов (region proposals) и оценки объектности (objectness scores), указывающие, насколько вероятно, что предложенный регион содержит объект. RPN использует якоря (anchors) — заранее определенные ограничивающие рамки различных масштабов и соотношений сторон, которые адаптируются в процессе обучения для лучшего предсказания регионов.\n",
    "\n",
    "- Якоря (anchors) в контексте детекции объектов с использованием сверточных нейронных сетей, таких как Faster R-CNN и его производные (например, SSD и YOLOv2 и далее), представляют собой предварительно определенные ограничивающие рамки (bounding boxes), которые служат отправными точками для предсказания реальных рамок объектов на изображении. Якоря позволяют модели эффективно находить объекты различных форм и размеров, адаптируя эти предварительно заданные рамки к конкретным объектам в процессе обучения.\n",
    "\n",
    "    ![](https://miro.medium.com/v2/resize:fit:506/0*udJNK6Tvzpf-5wOk.jpg)\n",
    "\n",
    "    1. **Определение размеров и соотношений**: Якоря определяются различными размерами и соотношениями сторон (aspect ratios), что позволяет покрыть широкий спектр возможных форм и размеров объектов. Например, для одной точки на карте признаков может быть определено несколько якорей разных размеров и соотношений сторон.\n",
    "\n",
    "    2. **Равномерное распределение по изображению**: Якоря равномерно распределяются по всему изображению, соответствуя каждой позиции на карте признаков, полученной после применения сверточных слоев к входному изображению. Это обеспечивает, что каждая область изображения будет рассмотрена моделью для возможного нахождения объектов.\n",
    "\n",
    "    3. **Предсказание смещений и классов**: Для каждого якоря модель предсказывает смещения относительно исходного положения якоря для точного позиционирования ограничивающей рамки вокруг объекта, а также вероятности принадлежности якоря к каждому из классов объектов.\n",
    "\n",
    "\n",
    "#### 2. Сеть детекции объектов\n",
    "\n",
    "После того как RPN предложила регионы, каждый регион корректируется и подается на вход дополнительной сверточной сети, которая выполняет классификацию объектов в предложенных регионах и уточняет их ограничивающие рамки. Эта часть похожа на Fast R-CNN, где используется RoI Pooling для приведения каждого предложенного региона к единому размеру перед подачей в полносвязные слои для классификации и регрессии рамок.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-24_at_5.10.31_PM.png)\n",
    "\n",
    "### Преимущества Faster R-CNN\n",
    "\n",
    "- **Высокая скорость и точность**: Интеграция RPN в модель позволяет Faster R-CNN детектировать объекты быстрее и точнее по сравнению с предыдущими методами.\n",
    "- **Универсальность**: Модель может быть обучена для детекции широкого спектра объектов без значительных изменений в архитектуре.\n",
    "- **End-to-end обучение**: Весь процесс, от предложения регионов до классификации и регрессии рамок, может быть обучен за один проход, что упрощает процесс обучения и улучшает производительность.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- **Вычислительные требования**: Несмотря на улучшения, Faster R-CNN все еще требует значительных вычислительных ресурсов для обучения и инференса, особенно на больших наборах данных.\n",
    "- **Сложность реализации**: Более сложная архитектура и процесс обучения могут представлять трудности для новичков в области глубокого обучения и компьютерного зрения.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*IZi_Jiv9l0cx4u2jji_4vw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a87f3-873e-4017-b218-efcb7c2400a1",
   "metadata": {},
   "source": [
    "### YOLO\n",
    "\n",
    "### Принцип работы YOLO\n",
    "\n",
    "YOLO рассматривает задачу детекции как проблему регрессии от пикселей изображения к пространственным ограничивающим рамкам и вероятностям классов. Изображение делится на сетку (например, 13x13 ячеек), и для каждой ячейки сетки модель предсказывает несколько ограничивающих рамок, вероятность наличия объекта в каждой рамке и вероятности принадлежности объекта к каждому классу.\n",
    "\n",
    "![](https://www.labellerr.com/blog/content/images/2023/01/yolo-algorithm-1.webp)\n",
    "\n",
    "#### Основные этапы:\n",
    "\n",
    "1. **Сегментация изображения на сетку**: Изображение делится на равные сегменты, для каждого из которых будут предсказаны ограничивающие рамки и вероятности классов.\n",
    "\n",
    "2. **Предсказание ограничивающих рамок и классов**: Для каждой ячейки сетки модель предсказывает N ограничивающих рамок и соответствующие им вероятности наличия объектов, а также условные вероятности принадлежности этих объектов к каждому из классов.\n",
    "\n",
    "3. **Фильтрация предсказаний**: Применяются механизмы для уменьшения количества ложных детекций и уточнения рамок, такие как пороговая фильтрация по вероятности и подавление немаксимумов (Non-maximum Suppression, NMS).\n",
    "\n",
    "#### Формат предсказаний\n",
    "\n",
    "![](https://dkharazi.github.io/bc56b1b7eb975f588af961996f1cfa5c/labelanchorbox.jpg)\n",
    "\n",
    "![](https://dkharazi.github.io/430e963578a6b7b33eee1ab32c341e27/gridoutput.jpg)\n",
    "\n",
    "### Преимущества YOLO\n",
    "\n",
    "- **Скорость**: YOLO может работать в реальном времени, что делает его подходящим для приложений, требующих быстрой обработки, таких как видеонаблюдение и водительские ассистенты.\n",
    "- **Обнаружение с контекстом**: Поскольку YOLO рассматривает всё изображение при предсказании каждой ограничивающей рамки, оно эффективно учитывает контекстную информацию, что может помочь уменьшить количество ложных детекций.\n",
    "- **Простота**: Архитектура YOLO относительно проста по сравнению с некоторыми другими методами детекции, что упрощает её реализацию и обучение.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- **Точность**: На ранних версиях YOLO точность, особенно в части локализации объектов, могла быть ниже, чем у более медленных, но более точных методов, таких как Faster R-CNN.\n",
    "- **Обработка мелких объектов**: YOLO может испытывать трудности с детекцией мелких объектов или объектов, которые часто встречаются группами.\n",
    "\n",
    "С тех пор было представлено несколько улучшений и итераций YOLO, включая YOLOv2 (YOLO9000), YOLOv3, и далее до YOLOv5 и YOLOv6, каждая из которых вносит улучшения в точность, скорость и способность обнаруживать объекты различных размеров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63716997-1637-4523-b5a5-06882d8e9646",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "Vision Transformer (ViT) представляет собой инновационный подход к обработке изображений с использованием архитектуры трансформера, который до этого преимущественно применялся в области обработки естественного языка (NLP). Модель ViT была представлена исследователями из Google Research в статье \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" в 2020 году. Ключевая идея ViT заключается в применении архитектуры трансформера непосредственно к изображениям, предварительно разделенным на патчи (фрагменты), что позволяет эффективно захватывать глобальные зависимости между различными частями изображения.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
    "\n",
    "### Как работает Vision Transformer\n",
    "\n",
    "1. **Подготовка изображения**: Изображение разбивается на набор патчей (например, 16x16 пикселей). Эти патчи обрабатываются аналогично словам в тексте для NLP-задач.\n",
    "\n",
    "2. **Линейное проецирование**: Каждый патч преобразуется в вектор признаков с помощью линейного проецирования. К этим векторам добавляется позиционное кодирование для сохранения информации о пространственном расположении патчей в изображении.\n",
    "\n",
    "3. **Трансформер**: Проецированные векторы патчей подаются на вход трансформеру. Трансформер использует механизмы внимания, чтобы модель могла учиться на зависимостях между всеми патчами изображения, независимо от их пространственного расположения.\n",
    "\n",
    "4. **Классификация**: На выходе трансформера используется дополнительный токен [CLS], аналогичный использованию в задачах NLP, который проходит через все слои трансформера и используется для предсказания класса изображения.\n",
    "\n",
    "### Преимущества Vision Transformer\n",
    "\n",
    "- **Глобальное понимание изображения**: Благодаря механизмам внимания трансформер способен учитывать взаимосвязи между всеми частями изображения, что позволяет лучше понимать глобальный контекст.\n",
    "- **Масштабируемость**: ViT показывает улучшение производительности при увеличении объема тренировочных данных и размера модели, аналогично тому, как это наблюдается в NLP.\n",
    "- **Гибкость архитектуры**: Трансформеры могут быть легко адаптированы к различным задачам компьютерного зрения, включая классификацию изображений, сегментацию и детекцию объектов.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- **Требования к ресурсам**: Обучение ViT с нуля требует значительных вычислительных ресурсов и больших объемов данных. Однако предобученные модели могут быть эффективно дообучены на меньших наборах данных.\n",
    "- **Отсутствие явного понимания пространственных связей**: Несмотря на позиционное кодирование, изначальное представление патчей не включает явное понимание пространственных отношений между ними, что может быть недостатком по сравнению с сверточными сетями.\n",
    "\n",
    "\n",
    "![](https://viso.ai/wp-content/uploads/2021/09/attention-map-vision-transformers-vit-1060x558.jpg)\n",
    "\n",
    "![](https://xuranpan.plus/publication/dat/featured_hu2a5919fcf9b44331e6d70800bb6ac312_1031328_720x2500_fit_q75_h2_lanczos_3.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e444b93-8f29-470a-a484-aae92202ed42",
   "metadata": {},
   "source": [
    "# SAM\n",
    "\n",
    "[SAM](https://segment-anything.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
